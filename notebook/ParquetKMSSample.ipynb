{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0cf69fd-0511-492b-a9e6-1f35122d92d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Parquet Modular Encryption With Azure Key Vault KMS\n",
    "\n",
    "This example notebook should give you a quick glimpse about how to incorporate Parquet Modular Encryption with your Spark dataframes and Spark SQL commands. This notebook assumes that you have already created an Azure service principal that has access to Key Vault keys (https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal), and you have a configured Spark environment with the required Java class for Key Vault KMS Operations (https://github.com/Azure/parquet-modular-encryption-keyvault-kms), as well as the other Spark environment settings.\n",
    "\n",
    "First, let's ready in some sample data. In this example, the data is being read from an Azure storage account that contains the CitiBike data for New York City. This is a public dataset that you can download yourself from here: https://citibikenyc.com/system-data Then, place the data on your storage account. You can then subsitute the storage container name and storage account name as needed. This example uses the Data Lake Gen2 interface, so make sure your storge account is enabled for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a3ae86-61f7-48bf-bc63-6e07d8452853",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set your storage related variables here. Note that your storage_container_name and output_encrypted container name can be the same (see below).\n",
    "storage_account_name = \"storageaccountname\"\n",
    "storage_container_name = \"containername\"\n",
    "output_encrypted_container_name = \"anothercontainername\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1cb85c8-f1ce-46b9-8c4d-1ad4da8c8a36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read in the sample citibike data. This assumes you have CSV files from the data source noted in the header cell in a folder named 'citibike' in the root of the storage container\n",
    "raw_df = spark.read.format(\"csv\").option(\"header\",True).load(\"abfss://{0}@{1}.dfs.core.windows.net/citibike/*.csv\".format(storage_container_name, storage_account_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9473ab0b-271a-496a-a0ad-fd53dd27c8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Encryption\n",
    "\n",
    "To encrypt your parquet files, you need to provide options to your ```.write``` operation, specifying the key to use for your footer, as well as the key(s) to use on the column(s) you wish to encrypt. In this example, we are going to encrypt the ```ride_id``` column using a key from our KeyVault named ```columnKey```. The key identifier should in the format of key/versionID, unless your class can take in a key name and return a current version. The sample library does not do this. The target location here is also being written to a seperate container for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b415b79-dfa6-409a-b10e-70b5a941af7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Note: replace your key names and versions in the following command (<key_name> and <key_version> should be whole values, without the greater than and less than signs. Be careful not to remove the slash!)\n",
    "#This will also output (and overwrite) to a folder named \"encryptionDemo\" in your output storage container.\n",
    "raw_df.write.mode(\"overwrite\").option(\"parquet.encryption.footer.key\",\"<key name>/<key version>\").option(\"parquet.encryption.column.keys\",\"<key name>/<key version>:ride_id\").format(\"parquet\").save(\"abfss://{0}@{1}.dfs.core.windows.net/encryptionDemo/encryptedFooterExample\".format(output_encrypted_container_name,storage_account_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a1ce39-6625-4524-be80-6b2999e76556",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Decryption\n",
    "\n",
    "On a properly configured cluster, decryption is automatic on read. Just read in your data via Spark or Spark SQL and you're good to go! If you run this on a cluster without the proper configuraiton, you will get a \"No Keys Found\" exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb84ab55-9bee-4fcd-ab70-af0c7f42f569",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read the encrypted Parquet back out. To see what happens when someone tries to read the parquet without a properly configured cluster, try running this command on one that isn't\n",
    "encrypted_df = spark.read.format(\"parquet\").load(\"abfss://{0}@{1}.dfs.core.windows.net/encryptionDemo/encryptedFooterExample\".format(output_encrypted_container_name, storage_account_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e12d6dd-2df2-4b65-8c0e-9aacfae37754",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(encrypted_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1420252042765096,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Parquet KMS Sample",
   "notebookOrigID": 2723059039433029,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
